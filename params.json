{"name":"Parquet-format","tagline":"Columnar file format for hadoop","body":"Parquet\r\n======\r\nParquet is a columnar storage format for Hadoop.\r\n\r\nWe created Parquet to make the advantages of compressed, efficient columnar data representation available to any project in the Hadoop ecosystem.\r\n\r\nParquet is built from the ground up with complex **nested data structures** in mind, and uses the repetition/definition level approach to encoding such data structures, as popularized by Google Dremel. We believe this approach is superior to simple flattening of nested name spaces.\r\n\r\nParquet is built to support **very efficient compression and encoding schemes**. Multiple projects have demonstrated the performance impact of applying the right compression and encoding scheme to the data. Parquet allows compression schemes to be specified on a per-column level, and is future-proofed to allow adding more encodings as they are invented and implemented. \r\n\r\nParquet is built to be used by anyone. The Hadoop ecosystem is rich with data processing frameworks, and we are not interested in playing favorites. We believe that an efficient, well-implemented columnar storage substrate should be **useful to all frameworks without the cost of extensive and difficult to set up dependencies**. \r\n\r\nParquet is, of course, available under the Apache License v2.0.\r\n\r\n# Implementation Details\r\n\r\nParquet metadata is encoded using Apache Thrift.\r\n\r\nThe Parquet-format project contains all Thrift definitions that are necessary to create readers\r\nand writers for Parquet files. \r\n\r\n## Glossary\r\n  - Block (hdfs block): This means a block in hdfs and the meaning is \r\n    unchanged for describing this file format.  The file format is \r\n    designed to work well ontop of hdfs.\r\n\r\n  - File: A hdfs file that must include the metadata for the file.\r\n    It does not need to actually contain the data.\r\n\r\n  - Row group: A logical horizontal partitioning of the data into rows.\r\n    There is no physical structure that is guaranteed for a row group.\r\n    A row group consists of a column chunk for each column in the dataset.\r\n\r\n  - Column chunk: A chunk of the data for a particular column.  These live\r\n    in a particular row group and is guaranteed to be contiguous in the file.\r\n\r\n  - Page: Column chunks are divided up into pages.  A page is conceptually\r\n    an indivisible unit (in terms of compression and encoding).  There can\r\n    be multiple page types which is interleaved in a column chunk.\r\n\r\nHierarchically, a file consists of one or more rows groups.  A row group\r\ncontains exactly one column chunk per column.  Column chunks contain one or\r\nmore pages. \r\n\r\n## Unit of parallelization\r\n  - MapReduce - File/Row Group\r\n  - IO - Column chunk\r\n  - Encoding/Compression - Page\r\n\r\n## File format\r\nThis file and the thrift definition should be read together to understand the format.\r\n\r\n    4-byte magic number \"RED1\"\r\n    <Column 1 Chunk 1 + Column Metadata>\r\n    <Column 2 Chunk 1 + Column Metadata>\r\n    ...\r\n    <Column N Chunk 1 + Column Metadata>\r\n    <Column 1 Chunk 2 + Column Metadata>\r\n    <Column 2 Chunk 2 + Column Metadata>\r\n    ...\r\n    <Column N Chunk 2 + Column Metadata>\r\n    ...\r\n    <Column 1 Chunk M + Column Metadata>\r\n    <Column 2 Chunk M + Column Metadata>\r\n    ...\r\n    <Column N Chunk M + Column Metadata>\r\n    File Metadata\r\n    4-byte offset from end of file to start of file metadata\r\n    4-byte magic number \"RED1\"\r\n\r\nIn the above example, there are N columns in this table, split into M row \r\ngroups.  The file metadata contains the locations of all the column metadata \r\nstart locations.  More details on what is contained in the metdata can be found \r\nin the thrift files.\r\n\r\nMetadata is written after the data to allow for single pass writing.\r\n\r\nReaders are expected to first read the file metadata to find all the column \r\nchunks they are interested in.  The columns chunks should then be read sequentially.\r\n\r\n## Metadata\r\nThere are three types of metadata: file metadata, column (chunk) metadata and page\r\nheader metadata.  All thrift structures are serialized using the TCompactProtocol.\r\n\r\n## Types\r\nThe types supported by the file format are intended to be as minimal as possible,\r\nwith a focus on how the types effect on disk storage.  For example, 16-bit ints\r\nare not explicitly supported in the storage format since they are covered by\r\n32-bit ints with an efficient encoding.  This reduces the complexity of implementing\r\nreaders and writers for the format.  The types are:\r\n  - BOOLEAN: 1 bit boolean\r\n  - INT32: 32 bit signed ints\r\n  - INT64: 64 bit signed ints\r\n  - INT96: 96 bit signed ints\r\n  - FLOAT: IEEE 32-bit floating point values\r\n  - DOUBLE: IEEE 64-bit floating point values\r\n  - BYTE_ARRAY: arbitrarily long byte arrays.\r\n\r\n## Nested Encoding\r\nTo encode nested columns, Parquet uses the dremel encoding with definition and \r\nrepetition levels.  Definition levels specify how many optional fields in the \r\npath for the column are defined.  Repetition levels specify at what repeated field\r\nin the path has the value repeated.  The max definition and repetition levels can\r\nbe computed from the schema (i.e. how much nesting is there).  This defines the\r\nmaximum number of bits required to store the levels (levels are defined for all\r\nvalues in the column).  \r\n\r\nFor the definition levels, the values are encoded using run length encoding.\r\nThe run length encoding is serialized as follows:\r\n - let max be the maximum definition level (determined by the schema)\r\n - let w be the width in bits required to encode a definition level value. w = ceil(log2(max + 1))\r\n - If the value is repeated we store:\r\n  - 1 as one bit\r\n  - the value encoded in w bits\r\n  - the repetition count as an unsigned var int. (see ULEB128: http://en.wikipedia.org/wiki/Variable-length_quantity)\r\n - If the value is not repeated (or not repeated enough so that the above scheme would be more compact)\r\n  - 0 as one bit\r\n  - the value encoded in w bits\r\n\r\nTo sum up:\r\n - the first bit is 1 if we're storing a repeated value [1][value][count]\r\n - it is 0 if we're storing the value without repetition count [0][value]\r\n - 0 or 1 is stored as 1 bit\r\n - value is stored as w bits\r\n - count is stored as var int\r\n\r\nFor repetition levels, the levels are bit packed as tightly as possible, \r\nrounding up to the nearest byte.  For example, if the max repetition level was 3\r\n(2 bits) and the max definition level as 3 (2 bits), to encode 30 values, we would\r\nhave 30 * 2 = 60 bits = 8 bytes.\r\n\r\n## Nulls\r\nNullity is encoded in the definition levels (which is run-length encoded).  NULL values \r\nare not encoded in the data.  For example, in a non-nested schema, a column with 1000 NULLs \r\nwould be encoded with run-length encoding (0, 1000 times) for the definition levels and\r\nnothing else.  \r\n\r\n## Data Pages\r\nFor data pages, the 3 pieces of information are encoded back to back, after the page\r\nheader.  We'll have the definition levels, followed by repetition levels, followed\r\nby the encoded values.  The size of specified in the header is for all 3 pieces combined.\r\n\r\n## Column chunks\r\nColumn chunks are composed of pages written back to back.  The pages share a common \r\nheader and readers can skip over page they are not interested in.  The data for the \r\npage follows the header and can be compressed and/or encoded.  The compression and \r\nencoding is specified in the page metadata.\r\n\r\n## Checksumming\r\nData pages can be individually checksummed.  This allows disabling of checksums at the \r\nHDFS file level, to better support single row lookups.\r\n\r\n## Error recovery\r\nIf the file metadata is corrupt, the file is lost.  If the column metdata is corrupt, \r\nthat column chunk is lost (but column chunks for this column in order row groups are \r\nokay).  If a page header is corrupt, the remaining pages in that chunk are lost.  If \r\nthe data within a page is corrupt, that page is lost.  The file will be more \r\nresilient to corruption with smaller row groups.\r\n\r\nPotential extension: With smaller row groups, the biggest issue is lowing the file \r\nmetadata at the end.  If this happens in the write path, all the data written will \r\nbe unreadable.  This can be fixed by writing the file metadata every Nth row group.  \r\nEach file metadata would be cumulative and include all the row groups written so \r\nfar.  Combining this with the strategy used for rc or avro files using sync markers, \r\na reader could recovery partially written files.  \r\n\r\n## Configurations\r\n- Row group size: Larger row groups allow for larger column chunks which makes it \r\npossible to do larger sequential IO.  Larger groups also require more buffering in \r\nthe write path (or a two pass write).  We recommend large row groups (512GB - 1GB).  \r\nSince an entire row group might need to be read, we want it to completely fit on \r\none HDFS block.  Therefore, HDFS block sizes should also be set to be larger.  An \r\noptimized read setup would be: 1GB row groups, 1GB HDFS block size, 1 HDFS block \r\nper HDFS file.\r\n- Data page size: Data pages should be considered indivisible so smaller data pages \r\nallow for more fine grained reading (e.g. single row lookup).  Larger page sizes \r\nincur less space overhead (less page headers) and potentially less parsing overhead \r\n(processing headers).  Note: for sequential scans, it is not expected to read a page \r\nat a time; this is not the IO chunk.  We recommend 8KB for page sizes.\r\n\r\n## Extensibility\r\nThere are many places in the format for compatible extensions:\r\n- File Version: The file metadata contains a version.\r\n- Encodings: Encodings are specified by enum and more can be added in the future.  \r\n- Page types: Additional page types can be added and safely skipped.\r\n\r\n## License\r\nCopyright 2013 Twitter, Cloudera and other contributors.\r\n\r\nLicensed under the Apache License, Version 2.0: http://www.apache.org/licenses/LICENSE-2.0\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}